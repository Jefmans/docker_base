The idea is to remove all unwanted text from a pdf. This is done page per page.
We also take image + captions out. These need to be saved (already in the system)
And then combine the text again in 1 text-file that will be used for chunking and embedding. 
We want to use a system that has the option for multiple chunking length. (and 20% overlap) (chunksize == 200 * n)
Then we do the embedding (also for the captions).

Our goal is to create an intelligent search system. 
We'll do this by vectorizing the query and searching for close embeddings (ex. top 10 chunksize 200 + top 10 chunksize 800). 
Although not 100% certain, we'll propbaly use an llm to generate the response. 
The response however should refer to the original texts. So we need to keep track of the pages as well. (during text concatenation)


Below you find the code that I already tested in a seperate system. 
How to use this in the project ?

clean_headers_footers.py:
from collections import defaultdict, Counter
from typing import List, Dict
import fitz
from typing import List, Tuple

def normalize(line: str) -> str:
    return line.strip().lower()

def detect_repeating_lines_next_pages(
    pages_text: List[List[str]],
    n: int = 5
) -> List[Tuple[List[str], List[str]]]:
    """
    For each page, return (header_lines, footer_lines) if any of them repeat
    exactly in the next 1â€“2 pages.
    """
    results = []

    for i, page in enumerate(pages_text):
        top_lines = [normalize(line) for line in page[:n]]
        bottom_lines = [normalize(line) for line in page[-n:]]

        header_matches = set()
        footer_matches = set()

        for j in [1, 2]:  # lookahead: next page and one after
            if i + j >= len(pages_text):
                continue

            next_page = pages_text[i + j]
            next_top = [normalize(line) for line in next_page[:n]]
            next_bottom = [normalize(line) for line in next_page[-n:]]

            # Exact match for header lines
            for line in top_lines:
                if line and line in next_top:
                    header_matches.add(line)

            # Exact match for footer lines
            for line in bottom_lines:
                if line and line in next_bottom:
                    footer_matches.add(line)

        results.append((
            list(header_matches),
            list(footer_matches)
        ))

    return results

from typing import List, Set
from rapidfuzz import fuzz

def normalize(line: str) -> str:
    return line.strip().lower()

def collect_repeating_lines(
    pages_text: List[List[str]],
    n: int = 5,
    lookahead: int = 2,
    threshold: int = 100  # exact match = 100, or set to 85 for fuzzy
) -> (Set[str], Set[str]):
    """
    Scans all pages. For each page, checks top/bottom `n` lines against the next `lookahead` pages.
    If any line is repeated, it is added to the global header/footer sets.
    """
    header_candidates = set()
    footer_candidates = set()

    for i, page in enumerate(pages_text):
        top_lines = [normalize(line) for line in page[:n]]
        bottom_lines = [normalize(line) for line in page[-n:]]

        for j in range(1, lookahead + 1):
            if i + j >= len(pages_text):
                break

            next_top = [normalize(line) for line in pages_text[i + j][:n]]
            next_bottom = [normalize(line) for line in pages_text[i + j][-n:]]

            for line in top_lines:
                if line and any(fuzz.ratio(line, other) >= threshold for other in next_top):
                    header_candidates.add(line)

            for line in bottom_lines:
                if line and any(fuzz.ratio(line, other) >= threshold for other in next_bottom):
                    footer_candidates.add(line)

    return header_candidates, footer_candidates


from typing import List, Set

def remove_repeating_lines(
    pages_text: List[List[str]],
    header_set: Set[str],
    footer_set: Set[str],
    n: int = 5
) -> List[str]:
    """
    Removes lines from each page if they match a known header/footer line
    (within top/bottom `n` lines).
    Returns list of cleaned page strings.
    """
    cleaned_pages = []

    for page in pages_text:
        cleaned = []
        for i, line in enumerate(page):
            norm = line.strip().lower()

            is_header_zone = i < n
            is_footer_zone = i >= len(page) - n

            if is_header_zone and norm in header_set:
                continue
            if is_footer_zone and norm in footer_set:
                continue

            cleaned.append(line)

        cleaned_pages.append("\n".join(cleaned))

    return cleaned_pages


if __name__ == "__main__":
    # file_path = 'pdfs/black_hole.pdf'
    # file_path = 'pdfs/de_witte.pdf'
    file_path = 'pdfs/astronomy.pdf'

    doc = fitz.open(file_path)
    pages_text = [page.get_text().splitlines() for page in doc[61:66]]

    # Step 1: Detect repeated lines
    header_set, footer_set = collect_repeating_lines(pages_text, n=5)

    # Step 2: Remove them from the text
    cleaned_pages = remove_repeating_lines(pages_text, header_set, footer_set, n=5)

    # Output
    for i, page in enumerate(cleaned_pages):
        print(f"\n--- Page {i + 1} ---\n{page}")



clean_page_numbers.py:
import fitz  # PyMuPDF
from difflib import SequenceMatcher
import re
from rapidfuzz import fuzz
import re

def is_arabic_number(s):
    return re.fullmatch(r"\s*\d{1,4}\s*", s) is not None

def is_roman_number(s):
    return re.fullmatch(r"\s*[ivxlcdmIVXLCDM]{1,7}\s*", s) is not None

def roman_to_int(s):
    roman_map = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}
    s = s.upper()
    result = 0
    prev_value = 0
    for char in reversed(s):
        value = roman_map.get(char, 0)
        if value < prev_value:
            result -= value
        else:
            result += value
            prev_value = value
    return result if result > 0 else None

def detect_page_numbers(pages_text, n=3, min_sequence_len=4):
    candidates = []  # list of dicts: {"index": int, "line": str, "pos": "top"/"bottom", "value": int}
    
    for idx, lines in enumerate(pages_text):
        top_lines = lines[:n]
        bottom_lines = lines[-n:] if len(lines) >= n else lines

        for pos, group in [('top', top_lines), ('bottom', bottom_lines)]:
            for line in group:
                stripped = line.strip()
                if is_arabic_number(stripped):
                    value = int(stripped)
                elif is_roman_number(stripped):
                    value = roman_to_int(stripped)
                    if value is None:
                        continue
                else:
                    continue

                candidates.append({
                    "index": idx,
                    "line": line,
                    "pos": pos,
                    "value": value
                })

    # Group by position ("top" or "bottom")
    sequences = []
    for pos in ['top', 'bottom']:
        group = [c for c in candidates if c["pos"] == pos]
        group.sort(key=lambda x: x["index"])

        temp_seq = []
        for i, entry in enumerate(group):
            if not temp_seq:
                temp_seq.append(entry)
            else:
                prev = temp_seq[-1]
                # Allow gaps, but ensure ordering
                if entry["index"] > prev["index"] and entry["value"] > prev["value"]:
                    temp_seq.append(entry)
                elif len(temp_seq) >= min_sequence_len:
                    sequences.append(temp_seq)
                    temp_seq = [entry]
                else:
                    temp_seq = [entry]
        if len(temp_seq) >= min_sequence_len:
            sequences.append(temp_seq)

    return sequences


def remove_page_numbers(pages_text, sequences, n=3):
    cleaned = []
    skip_lines_per_page = {}  # page_index -> list of lines to remove

    for seq in sequences:
        for item in seq:
            page = item["index"]
            line = item["line"].strip()
            skip_lines_per_page.setdefault(page, []).append(line)

    for i, lines in enumerate(pages_text):
        to_remove = skip_lines_per_page.get(i, [])
        cleaned_lines = [line for line in lines if line.strip() not in to_remove]
        cleaned.append("\n".join(cleaned_lines))

    return cleaned



if __name__ == "__main__":
    file_path = 'pdfs/black_hole.pdf'
    # file_path = 'pdfs/de_witte.pdf'

    doc = fitz.open(file_path)
    pages_text = [page.get_text().splitlines() for page in doc[61: 66]]
    # pages_text = [page.get_text().splitlines() for page in doc]

    # Detect and clean page numbers
    sequences = detect_page_numbers(pages_text)
    cleaned_pages = remove_page_numbers(pages_text, sequences)

    for page in cleaned_pages:
        print(page)
        print('---------------------------')





