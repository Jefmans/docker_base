I think we're getting close. But it's not 100% what I have in mind. The flow we should try to achieve is:

1) User creates query
2) We find chunks for this query
3) we use these chunks to create a root-outline
4) we use these chunks + outline to create sub-questions per part of the root-outline
5) we search chunks per part based on the sub-questions
6) create a sub-outline for this part (if needed) => repeat 4 +5 for these new sub-outline
7) create content per (sub) part based on the useful chunks 
8) combine all together
9) add executive summary + conclusion


need to think about:
a) we should check if a current part is 
         i. complete => does it need more questions / chunks to expand?
         ii. no overlap with the rest of the article => no duplication of same content - although some parts can be repeated in another context of course
b) 


In all this we should build an Object that is keeping track of all the data (chunks, questions, outline, hierarchy (root, sub, sub-sub, ...) , created texts, ...)
This object should also be able to create a complete article based on this data. In html or latex, ... 
this object should also be able to check if there is redundant data. If we for example create sub questions, these should be checked if already used somewhere else. Same for the chunks. => optimize data usage
This object should already have some structure in it, like the executive summary + conclusion (maybe even per part/section)


Give me your remarks + plan 


Below you can find the structure + current code:

C:.
│   .env
│   .gitignore
│   docker-compose.yml
│   README.md
│
├───backend
│   │   Dockerfile
│   │   requirements.txt
│   │
│   └───app
│       │   db.py
│       │   main.py
│       │   schemas.py
│       │
│       ├───models
│       │       image_model.py
│       │       research_tree.py
│       │
│       ├───routers
│       │       agent.py
│       │       extract.py
│       │       health.py
│       │       process.py
│       │       query.py
│       │       query_agent.py
│       │       upload.py
│       │
│       └───utils
│           │   save_images.py
│           │   vectorstore.py
│           │
│           └───agent
│                   finalizer.py
│                   memory.py
│                   outline.py
│                   search_chunks.py
│                   subquestions.py
│                   writer.py
│
├───docker
│   └───pgadmin
│           servers.json
│
└───pdf_worker
    │   Dockerfile
    │   requirements.txt
    │
    └───app
        │   main.py
        │   models.py
        │
        └───utils
            │   embedding.py
            │   embed_captions.py
            │   es.py
            │   image_extraction.py
            │   metadata.py
            │   pdf_pipeline.py
            │   pdf_reader.py
            │   text_chunker.py
            │
            └───cleaning
                    clean_text_pipeline.py
                    header_footer.py
                    page_numbers.py



docker-compse.yml file :

services:
  traefik:
    image: traefik:latest
    container_name: traefik
    restart: always
    command:
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--api.dashboard=true"
      - "--api.insecure=true"  # Only for local/dev
    ports:
      - "80:80"
    networks:
      - internal_backend
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      - "traefik.enable=true"
      # - "traefik.http.routers.traefik.rule=PathPrefix(`/dashboard`)"
      - "traefik.http.routers.traefik.rule=PathPrefix(`/dashboard`) || PathPrefix(`/api`)"
      # - "traefik.http.routers.traefik.rule=PathPrefix(`/dashboard`) || PathPrefix(`/traefik-api`)"
      - "traefik.http.routers.traefik.entrypoints=web"
      - "traefik.http.routers.traefik.service=api@internal"

  whoami:
    image: traefik/whoami
    container_name: whoami
    restart: always
    networks:
      - internal_backend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.whoami.rule=PathPrefix(`/whoami`)"
      - "traefik.http.routers.whoami.entrypoints=web"


  portainer_agent:
    image: portainer/agent:latest
    container_name: portainer_agent
    restart: always
    ports:
      - "9001:9001"  # Insecure if exposed to all IPs!
    networks:
      - internal_backend
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/volumes:/var/lib/docker/volumes


  postgres:
    image: postgres:latest
    container_name: postgres
    restart: always
    environment:
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
      POSTGRES_DB: testdb
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - internal_backend
    labels:
      # - "traefik.enable=true"
      # - "traefik.http.routers.postgres.rule=PathPrefix(`/db`)"
      - "traefik.http.routers.postgres.entrypoints=web"
      - "traefik.http.routers.postgres.middlewares=db-strip"
      - "traefik.http.middlewares.db-strip.stripprefix.prefixes=/db"
      - "traefik.http.services.postgres.loadbalancer.server.port=5432"  # not an HTTP port, only for test purposes


  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin123
    ports:
      - "8081:80" # Temporary Hack): Expose a dedicated port => need a domain
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./docker/pgadmin/servers.json:/pgadmin4/servers.json
    networks:
      - internal_backend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.pgadmin.rule=PathPrefix(`/pgadmin`)"
      - "traefik.http.routers.pgadmin.entrypoints=web"
      - "traefik.http.routers.pgadmin.middlewares=pgadmin-strip"
      - "traefik.http.middlewares.pgadmin-strip.stripprefix.prefixes=/pgadmin"
      - "traefik.http.services.pgadmin.loadbalancer.server.port=80"


  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
    container_name: elasticsearch
    restart: always
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elastic_data:/usr/share/elasticsearch/data
    networks:
      - internal_backend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.elasticsearch.rule=PathPrefix(`/es`)"
      - "traefik.http.routers.elasticsearch.entrypoints=web"
      - "traefik.http.routers.elasticsearch.middlewares=es-strip"
      - "traefik.http.middlewares.es-strip.stripprefix.prefixes=/es"
      - "traefik.http.services.elasticsearch.loadbalancer.server.port=9200"


  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.2
    container_name: kibana
    restart: always
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_BASEPATH=/kibana
      - SERVER_REWRITEBASEPATH=true
    depends_on:
      - elasticsearch
    networks:
      - internal_backend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kibana.rule=PathPrefix(`/kibana`)"
      - "traefik.http.routers.kibana.entrypoints=web"

      # - "traefik.http.middlewares.kibana-redirect.redirectregex.regex=^/kibana$$"
      # - "traefik.http.middlewares.kibana-redirect.redirectregex.replacement=/kibana/app/home"
      # - "traefik.http.middlewares.kibana-redirect.redirectregex.permanent=true"
      - "traefik.http.services.kibana.loadbalancer.server.port=5601"



  backend:
    env_file:
      - .env
    build:
      context: ./backend
    container_name: backend
    restart: always
    networks:
      - internal_backend
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.backend.rule=PathPrefix(`/backend`)"
      - "traefik.http.routers.backend.entrypoints=web"
      - "traefik.http.services.backend.loadbalancer.server.port=8000"

  minio:
    image: minio/minio:latest
    container_name: minio
    restart: always
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - minio_data:/data
    networks:
      - internal_backend
    ports:
      - "9101:9001"       


  # pdf_reader:
  #   build:
  #     context: ./pdf_worker
  #   container_name: pdf_reader
  #   restart: always
  #   environment:
  #     PDF_NAME: "5a4aac01-9c03-462b-955a-be69ca5f94e5_de_witte.pdf"  # Set to a known PDF in MinIO
  #   networks:
  #     - internal_backend


  pdf_worker:
    env_file:
      - .env
    build:
      context: ./pdf_worker
    container_name: pdf_worker
    restart: always
    networks:
      - internal_backend
    depends_on:
      - minio
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.pdfworker.rule=PathPrefix(`/pdfworker`)"
      - "traefik.http.routers.pdfworker.entrypoints=web"
      - "traefik.http.routers.pdfworker.middlewares=strip-worker"
      - "traefik.http.middlewares.strip-worker.stripprefix.prefixes=/pdfworker"
      - "traefik.http.services.pdfworker.loadbalancer.server.port=8000"



volumes:
  postgres_data:
  pgadmin_data:
  elastic_data:
  minio_data:





networks:
  internal_backend:
    name: internal_backend
    driver: bridge





===== .\backend\app\models\image_model.py =====
from pydantic import BaseModel
from typing import List, Optional

class ImageCreate(BaseModel):
    book_id: str
    source_pdf: str
    page_number: int
    xref: int
    filename: str
    caption: Optional[str] = ""
    embedding: Optional[List[float]] = None



===== .\backend\app\models\research_tree.py =====
# core/agent/research_tree.py

from typing import List, Optional, Dict
from pydantic import BaseModel


class Chunk(BaseModel):
    id: str
    text: str
    page: Optional[int]
    source: Optional[str]
    embedding: Optional[List[float]] = None


class ResearchNode(BaseModel):
    title: str
    questions: List[str] = []
    chunks: List[Chunk] = []
    content: Optional[str] = None
    summary: Optional[str] = None
    conclusion: Optional[str] = None

    parent: Optional["ResearchNode"] = None
    subnodes: List["ResearchNode"] = []

    class Config:
        arbitrary_types_allowed = True
        underscore_attrs_are_private = True
        json_encoders = {
            "ResearchNode": lambda v: v.dict(exclude_none=True)
        }

    def add_subnode(self, node: "ResearchNode"):
        node.parent = self
        self.subnodes.append(node)

    def all_chunks(self) -> List[Chunk]:
        return self.chunks + [c for sn in self.subnodes for c in sn.all_chunks()]

    def all_questions(self) -> List[str]:
        return self.questions + [q for sn in self.subnodes for q in sn.all_questions()]

    def find_node_by_title(self, title: str) -> Optional["ResearchNode"]:
        if self.title == title:
            return self
        for sn in self.subnodes:
            result = sn.find_node_by_title(title)
            if result:
                return result
        return None


class ResearchTree(BaseModel):
    query: str
    root_node: ResearchNode

    class Config:
        arbitrary_types_allowed = True

    def deduplicate_chunks(self):
        seen_ids = set()
        def dedup(node: ResearchNode):
            unique = []
            for c in node.chunks:
                if c.id not in seen_ids:
                    unique.append(c)
                    seen_ids.add(c.id)
            node.chunks = unique
            for sn in node.subnodes:
                dedup(sn)
        dedup(self.root_node)

    def deduplicate_questions(self):
        seen = set()
        def dedup(node: ResearchNode):
            filtered = []
            for q in node.questions:
                qnorm = q.strip().lower()
                if qnorm not in seen:
                    filtered.append(q)
                    seen.add(qnorm)
            node.questions = filtered
            for sn in node.subnodes:
                dedup(sn)
        dedup(self.root_node)

    def generate_article(self, format: str = "markdown") -> str:
        if format == "markdown":
            return self._to_markdown()
        raise NotImplementedError(f"Format {format} not supported yet.")

    def _to_markdown(self) -> str:
        def walk(node: ResearchNode, level: int = 2) -> str:
            text = f"{'#' * level} {node.title}\n\n"
            if node.content:
                text += node.content.strip() + "\n\n"
            if node.summary:
                text += f"**Summary:** {node.summary.strip()}\n\n"
            if node.conclusion:
                text += f"**Conclusion:** {node.conclusion.strip()}\n\n"
            for sn in node.subnodes:
                text += walk(sn, level + 1)
            return text

        return f"# Research Article\n\n## Query\n{self.query}\n\n" + walk(self.root_node)

    def all_nodes(self) -> List[ResearchNode]:
        nodes = []
        def walk(node: ResearchNode):
            nodes.append(node)
            for sn in node.subnodes:
                walk(sn)
        walk(self.root_node)
        return nodes



===== .\backend\app\routers\agent.py =====
from fastapi import APIRouter, Request, HTTPException, BackgroundTasks
from uuid import uuid4
from pydantic import BaseModel
from app.utils.agent.search_chunks import search_chunks
from app.utils.agent.memory import save_session_chunks, get_session_chunks, save_section
from app.utils.agent.subquestions import generate_subquestions_from_chunks
from app.utils.agent.outline import generate_outline, Outline
from app.utils.agent.writer import write_section
import json
from app.utils.agent.finalizer import finalize_article
from app.models.research_tree import ResearchTree, ResearchNode, Chunk

router = APIRouter()

class AgentQueryRequest(BaseModel):
    query: str = "What is a black hole ?"
    top_k: int = 5

@router.post("/agent/query")
async def start_query_session(request: AgentQueryRequest):
    user_query = request.query

    if not user_query:
        raise HTTPException(status_code=400, detail="Missing 'query' in request body.")

    try:
        # Search chunks in Elastic
        top_chunks = search_chunks(user_query, top_k=request.top_k)

        # Save in a new session
        session_id = str(uuid4())
        save_session_chunks(session_id, user_query, top_chunks)

        return {
            "status": "success",
            "session_id": session_id,
            "query": user_query,
            "preview_chunks": top_chunks[:3]  # optional preview
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



@router.post("/agent/subquestions")
def generate_subquestions(session_id: str):
    session = get_session_chunks(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    query = session["query"]
    chunks = session["chunks"]
    subq = generate_subquestions_from_chunks(chunks, query)

    return {
        "session_id": session_id,
        "query": query,
        "subquestions": subq
    }


@router.post("/agent/outline")
def create_outline(session_id: str):
    session = get_session_chunks(session_id)
    if not session or "query" not in session or "chunks" not in session:
        raise HTTPException(status_code=404, detail="Session not found")

    subq = generate_subquestions_from_chunks(session["chunks"], session["query"])
    outline = generate_outline(subq, session["query"])

    # âœ… Save it!
    session["outline"] = outline.dict()
    save_session_chunks(session_id, session["query"], session["chunks"])  # resave session with outline
    print(session)

    return {
        "session_id": session_id,
        "outline": outline.dict()
    }


@router.post("/agent/section/{section_id}")
def write_section_by_id(session_id: str, section_id: int):
    session = get_session_chunks(session_id)
    # print(session)
    if not session or "outline" not in session:
        raise HTTPException(status_code=404, detail="Session or outline missing")

    outline_data = session["outline"]
    if isinstance(outline_data, str):
        outline_data = json.loads(outline_data)  # support old string-based storage

    try:
        outline = Outline(**outline_data)
        section = outline.sections[section_id]
        generated_text = write_section(section.dict())
        save_section(session_id, section_id, generated_text)

        return {
            "session_id": session_id,
            "section_id": section_id,
            "heading": section.heading,
            "text": generated_text
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/agent/article/finalize")
def finalize_article_route(session_id: str):
    session = get_session_chunks(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")

    # Make sure session includes the sections
    session["session_id"] = session_id  # Needed for get_all_sections
    article_text = finalize_article(session)

    return {
        "session_id": session_id,
        "title": session.get("outline", {}).get("title", "Untitled Article"),
        "article": article_text
    }



@router.post("/agent/full_run")
def full_run(request: AgentQueryRequest, background_tasks: BackgroundTasks = None):
    try:
        # STEP 1: Query & retrieve chunks
        session_id = str(uuid4())
        top_chunks = search_chunks(request.query, top_k=request.top_k)
        save_session_chunks(session_id, request.query, top_chunks)

        # STEP 2: Subquestions
        subq = generate_subquestions_from_chunks(top_chunks, request.query)

        # STEP 3: Outline
        outline = generate_outline(subq, request.query)
        session = get_session_chunks(session_id)
        session["outline"] = outline.dict()

        # STEP 4: Write each section
        section_outputs = []
        for i, section in enumerate(outline.sections):
            text = write_section(section.dict())
            save_section(session_id, i, text)
            section_outputs.append({
                "heading": section.heading,
                "text": text
            })

        # STEP 5: Finalize article
        session["session_id"] = session_id
        article = finalize_article(session)

        return {
            "session_id": session_id,
            "title": outline.title,
            "abstract": outline.abstract,
            "outline": [s.dict() for s in outline.sections],
            "sections": section_outputs,
            "article": article
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



===== .\backend\app\routers\extract.py =====
from fastapi import APIRouter, HTTPException
from minio import Minio
import requests
import io
import fitz  # PyMuPDF

router = APIRouter()

MINIO_BUCKET = "uploads"
# UNSTRUCTURED_API_URL = "http://95.216.215.141:8000/general/v0/general"
UNSTRUCTURED_API_URL = "http://unstructured_custom:8000/parse/"


minio_client = Minio(
    "minio:9000",
    access_key="minioadmin",
    secret_key="minioadmin123",
    secure=False
)


def extract_first_5_pages(pdf_bytes: bytes) -> io.BytesIO:
    original_doc = None
    new_doc = None
    try:
        original_doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        new_doc = fitz.open()

        for i in range(min(5, len(original_doc))):
            new_doc.insert_pdf(original_doc, from_page=i, to_page=i)

        output = io.BytesIO()
        new_doc.save(output)
        output.seek(0)
        return output
    finally:
        if original_doc:
            original_doc.close()
        if new_doc:
            new_doc.close()


@router.post("/extract/")
def extract_text(filename: str):
    try:
        response = minio_client.get_object(MINIO_BUCKET, filename)
        file_data = response.read()
        file_stream = io.BytesIO(file_data)

        try:
            files = {"files": (filename, file_stream, "application/pdf")}
            res = requests.post(UNSTRUCTURED_API_URL, files=files)
        finally:
            file_stream.close()

        if res.status_code != 200:
            raise Exception(res.text)

        elements = res.json()
        text = "\n".join(e.get("text", "") for e in elements if e.get("text"))

        return {
            "filename": filename,
            "extracted_characters": len(text),
            "preview": text
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/extract/preview/")
def extract_preview(filename: str):
    try:
        response = minio_client.get_object(MINIO_BUCKET, filename)
        file_data = response.read()

        short_pdf = extract_first_5_pages(file_data)
        try:
            files = {"files": (filename, short_pdf, "application/pdf")}
            res = requests.post(UNSTRUCTURED_API_URL, files=files)
        finally:
            short_pdf.close()

        if res.status_code != 200:
            raise Exception(res.text)

        elements = res.json()
        text = "\n".join(e.get("text", "") for e in elements if e.get("text"))

        return {
            "filename": filename,
            "pages_processed": 5,
            "extracted_characters": len(text),
            "preview": text[:500]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



===== .\backend\app\routers\health.py =====
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
def health_check():
    return {"status": "ok"}



===== .\backend\app\routers\process.py =====
from fastapi import APIRouter, HTTPException, Depends
import requests
import uuid
from sqlalchemy.orm import Session
from app.db import SessionLocal, get_db
from app.utils.save_images import save_image_metadata_list
from app.db import Document, ImageRecord
from pydantic import BaseModel
from typing import List, Optional
from app.schemas import ImageMetadata  # âœ… This uses the new schema


router = APIRouter()

PDF_WORKER_URL = "http://pdf_worker:8000/pdfworker"

@router.post("/process/metadata/{filename}")
def process_metadata(filename: str):
    try:
        # Step 1: Extract metadata via worker
        response = requests.post(f"{PDF_WORKER_URL}/metadata/{filename}")
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail="Metadata extraction failed")
        metadata = response.json()

        # Step 2: Store in Postgres
        db = SessionLocal()
        doc = Document(
            id=uuid.uuid4(),
            filename=filename,
            title=metadata["title"],
            year=metadata["year"],
            type=metadata["type"],
            topic=metadata["topic"],
            authors=metadata.get("authors"),
            isbn=metadata.get("isbn"),
            doi=metadata.get("doi"),
            publisher=metadata.get("publisher")
        )
        db.add(doc)
        db.commit()
        db.close()

        return {
            "status": "success",
            "filename": filename,
            "title": metadata["title"],
            "year": metadata["year"]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))






@router.post("/process/images/{filename}")
def process_images_and_save(filename: str, db: Session = Depends(get_db)):
    try:
        # 1. Get image metadata from pdf_worker
        response = requests.post(f"http://pdf_worker:8000/images/{filename}")
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail="pdf_worker error: " + response.text)

        image_data = response.json()

        # 2. Validate with Pydantic schema
        metadata_list = [ImageMetadata(**entry) for entry in image_data]

        # 3. Save to PostgreSQL using ORM
        save_image_metadata_list(db, metadata_list)

        return {
            "status": "success",
            "count": len(metadata_list),
            "saved": [meta.filename for meta in metadata_list]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))








===== .\backend\app\routers\query.py =====
import os
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
# from langchain_community.vectorstores import ElasticsearchStore
from langchain_openai import OpenAIEmbeddings
from elasticsearch import Elasticsearch
from langchain_elasticsearch import ElasticsearchStore


# Elasticsearch connection
es = Elasticsearch("http://elasticsearch:9200")

# LangChain embedding model
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
)

# Vector store
vectorstore = ElasticsearchStore(
    es_connection=es,
    index_name="pdf_chunks",  # Replace with your real index
    embedding=embedding_model,
    # vector_field="embedding",  # â† this must match your ES field name
)

caption_store = ElasticsearchStore(
    es_connection=es,
    index_name="captions",
    embedding=embedding_model,
    # vector_field="embedding",
)

# FastAPI router
router = APIRouter()

# Request model
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5

# Route
# @router.post("/query/")
# async def query(request: QueryRequest):
#     try:
#         results = vectorstore.similarity_search(
#             query=request.query,
#             k=request.top_k
#         )
#         return {"results": [r.page_content for r in results]}
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"âŒ Query error: {str(e)}")
@router.post("/query/")
async def query(request: QueryRequest):
    try:
        text_results = vectorstore.similarity_search_with_score(query=request.query, k=request.top_k)
        caption_results = caption_store.similarity_search_with_score(query=request.query, k=request.top_k)

        return {
            "text_chunks": [
                {
                    # "text": r.page_content,
                    # "score": score,
                    # "pages": r.pages  # contains filename, pages, chunk_index etc.
                    "r" : r
                }
                for r  in text_results
            ],
            "captions": [
                {
                    # "caption": r.caption,
                    # "score": score,
                    # "metadata": r.metadata  # will include minio_path
                    "r" : r
                }
                for r  in caption_results
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"âŒ Query error: {str(e)}")





===== .\backend\app\routers\query_agent.py =====
import os
import json
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from elasticsearch import Elasticsearch
from langchain_elasticsearch import ElasticsearchStore

# --- Models ---
class AgentQueryRequest(BaseModel):
    query: str
    top_k: int = 5

# --- Setup ---
es = Elasticsearch("http://elasticsearch:9200")
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
)
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
)

vectorstore = ElasticsearchStore(
    es_connection=es,
    index_name="pdf_chunks",
    embedding=embedding_model,
)

caption_store = ElasticsearchStore(
    es_connection=es,
    index_name="captions",
    embedding=embedding_model,
)

router = APIRouter()

# --- Endpoint ---
@router.post("/query/agent")
async def query_agent(request: AgentQueryRequest):
    try:
        # Step 1: Retrieve initial context
        chunks = vectorstore.similarity_search(request.query, k=request.top_k)
        captions = caption_store.similarity_search(request.query, k=request.top_k)
        context = "\n\n".join([doc.page_content for doc in chunks + captions])

        # Step 2: Generate initial answer
        prompt = f"""You are a scientific assistant. Based on the following context, answer the user's question.

        === CONTEXT ===
        {context}

        === QUESTION ===
        {request.query}
        """
        initial_answer = llm.invoke(prompt).content.strip()

        # Step 3: Generate hypotheses/sub-questions
        followup_prompt = f"""
        Based on the original question and answer, generate 3 to 5 meaningful sub-questions or hypotheses to explore further.

        QUESTION: {request.query}
        ANSWER: {initial_answer}

        Respond with a JSON list of strings like:
        ["...", "...", "..."]
        """
        followup_raw = llm.invoke(followup_prompt).content
        try:
            sub_questions = json.loads(followup_raw)
        except:
            sub_questions = [q.strip("- ") for q in followup_raw.strip().split("\n") if q.strip()]

        # Step 4: Answer each sub-question
        sub_answers = []
        for q in sub_questions:
            docs = vectorstore.similarity_search(q, k=request.top_k)
            caption_docs = caption_store.similarity_search(q, k=request.top_k)
            combined_context = "\n\n".join([doc.page_content for doc in docs + caption_docs])
            sub_prompt = f"""
            Based on the following context, answer this sub-question:

            CONTEXT:
            {combined_context}

            QUESTION: {q}
            """
            answer = llm.invoke(sub_prompt).content.strip()
            sub_answers.append({"question": q, "answer": answer})

        # Step 5: Final synthesis
        synthesis_prompt = f"""
        You are a scientific agent. Combine the information below to give a final, structured answer to the original question.

        ORIGINAL QUESTION: {request.query}
        INITIAL ANSWER: {initial_answer}

        SUB-ANSWERS:
        {json.dumps(sub_answers, indent=2)}

        Return a concise yet complete answer.
        """
        final_answer = llm.invoke(synthesis_prompt).content.strip()

        return {
            "query": request.query,
            "initial_answer": initial_answer,
            "chunks": chunks,
            "captions": captions, 
            "sub_questions": sub_questions,
            "sub_answers": sub_answers,
            "final_answer": final_answer
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Agent failed: {str(e)}")



===== .\backend\app\routers\upload.py =====
from fastapi import APIRouter, UploadFile, File, HTTPException
from minio import Minio
import uuid
import io

router = APIRouter()

# Connect to MinIO
minio_client = Minio(
    "minio:9000",  # Container name + default port
    access_key="minioadmin",
    secret_key="minioadmin123",
    secure=False
)

BUCKET_NAME = "uploads"

@router.post("/upload/")
async def upload_file(file: UploadFile = File(...)):
    try:
        # Generate a unique file name
        unique_filename = f"{uuid.uuid4()}_{file.filename}"
        content = await file.read()

        # Wrap bytes in a stream
        stream = io.BytesIO(content)        

        minio_client.put_object(
            bucket_name=BUCKET_NAME,
            object_name=unique_filename,
            data=stream,
            length=len(content),
            content_type=file.content_type,
        )

        return {
            "filename": unique_filename,
            "link": f"/minio/uploads/{unique_filename}"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



===== .\backend\app\utils\agent\finalizer.py =====
from app.utils.agent.outline import Outline
import json

def finalize_article(session_data: dict) -> str:
    if "outline" not in session_data or "query" not in session_data:
        return "âŒ Missing outline or query."

    outline = Outline(**session_data["outline"]) if isinstance(session_data["outline"], dict) else Outline(**json.loads(session_data["outline"]))
    session_id = session_data["session_id"]
    all_sections = session_data.get("sections") or {}

    stitched = f"# {outline.title}\n\n"
    stitched += f"**Abstract:** {outline.abstract}\n\n"

    for i, section in enumerate(outline.sections):
        text = all_sections.get(i, "").strip()
        if not text:
            continue
        stitched += f"## {section.heading}\n\n{text}\n\n"

    stitched += "## Conclusion\n\n(Conclusion not generated. Add manually if needed.)"

    return stitched.strip()



===== .\backend\app\utils\agent\memory.py =====
from typing import Dict, List

_session_store: Dict[str, Dict] = {}

def save_session_chunks(session_id: str, query: str, chunks: List[str]):
    _session_store[session_id] = _session_store.get(session_id, {})
    _session_store[session_id].update({
        "query": query,
        "chunks": chunks
    })

def get_session_chunks(session_id: str) -> Dict:
    return _session_store.get(session_id, {})

def save_section(session_id: str, section_index: int, text: str):
    if session_id in _session_store:
        _session_store[session_id].setdefault("sections", {})[section_index] = text

def get_all_sections(session_id: str) -> List[str]:
    return list(_session_store.get(session_id, {}).get("sections", {}).values())



===== .\backend\app\utils\agent\outline.py =====
from typing import List, Dict
from langchain_openai import ChatOpenAI

from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI



class OutlineSection(BaseModel):
    heading: str
    goals: str
    questions: List[str]

class Outline(BaseModel):
    title: str
    abstract: str
    sections: List[OutlineSection]




def generate_outline(subquestions: List[str], query: str) -> Outline:
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    parser = PydanticOutputParser(pydantic_object=Outline)

    formatted_subq = "\n".join(f"- {q}" for q in subquestions)

    prompt = PromptTemplate(
        template="""
            You are a scientific writer assistant. Create a full outline for a scientific article based on the main question and subquestions below.

            MAIN QUESTION:
            {query}

            SUBQUESTIONS:
            {formatted_subq}

            {format_instructions}
            """,
        input_variables=["query", "formatted_subq"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    chain = prompt | llm | parser
    return chain.invoke({"query": query, "formatted_subq": formatted_subq})





===== .\backend\app\utils\agent\search_chunks.py =====
from typing import List
from app.utils.vectorstore import get_vectorstore

def search_chunks(query: str, top_k: int = 100) -> List[str]:
    vs = get_vectorstore()  # your existing LangChain Elastic setup
    results = vs.similarity_search(query, k=top_k)
    # print(results[0].dict())

    return [r.page_content for r in results]



===== .\backend\app\utils\agent\subquestions.py =====
from typing import List
from langchain_openai import ChatOpenAI
import json

from pydantic import BaseModel
from typing import List

class SubquestionList(BaseModel):
    questions: List[str]



from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableMap
from langchain_openai import ChatOpenAI
import json

def generate_subquestions_from_chunks(chunks: List[str], user_query: str, model_name: str = "gpt-4o") -> List[str]:
    llm = ChatOpenAI(model=model_name, temperature=0)

    # Limit chunk length to avoid context overflow
    context = "\n\n".join(chunks[:20])

    parser = PydanticOutputParser(pydantic_object=SubquestionList)

    prompt = PromptTemplate(
        template="""
            You are a scientific assistant. Based on the user query and real scientific context below, generate a list of 5â€“10 detailed subquestions.

            Only ask questions that are grounded in the context. Return your result using this format:
            {format_instructions}

            === USER QUESTION ===
            {query}

            === CONTEXT ===
            {context}
            """,
        input_variables=["query", "context"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    chain = prompt | llm | parser

    try:
        result = chain.invoke({"query": user_query, "context": context})
        return result.questions
    except Exception as e:
        print("âŒ Subquestion parsing failed:", e)
        raise



===== .\backend\app\utils\agent\writer.py =====
from langchain_openai import ChatOpenAI
from app.utils.vectorstore import get_vectorstore, get_caption_store
from typing import List
import logging

logger = logging.getLogger(__name__)
llm = ChatOpenAI(model="gpt-4o", temperature=0)

def get_context_for_questions(questions: List[str], top_k: int = 5) -> str:
    vectorstore = get_vectorstore()
    caption_store = get_caption_store()
    chunks = []

    for q in questions:
        text_hits = vectorstore.similarity_search(q, k=top_k)
        cap_hits = caption_store.similarity_search(q, k=top_k)
        chunks.extend(text_hits + cap_hits)

    unique_texts = list({doc.page_content for doc in chunks})
    return "\n\n".join(unique_texts[:20])  # limit

def write_section(section: dict) -> str:
    heading = section["heading"]
    goals = section["goals"]
    questions = section["questions"]

    context = get_context_for_questions(questions)
    if not context.strip():
        return f"(No relevant context found for section: {heading})"

    prompt = f"""
            You are a scientific writer. Write a detailed section titled "{heading}" with the following goals:

            {goals}

            Use only the CONTEXT below, which comes from academic PDFs. Do not add external knowledge.

            CONTEXT:
            {context}

            Write a clear and informative section (300â€“800 words) based on the questions:
            {questions}
            """
    return llm.invoke(prompt).content.strip()



===== .\backend\app\utils\save_images.py =====
from app.db import ImageRecord
from app.schemas import ImageMetadata

def save_image_metadata_list(db, metadata_list: list[ImageMetadata]):
    for meta in metadata_list:
        record = ImageRecord(
            book_id=meta.book_id,
            source_pdf=meta.source_pdf,
            page_number=meta.page_number,
            xref=meta.xref,
            filename=meta.filename,
            caption=meta.caption,
        )
        db.add(record)
    db.commit()



===== .\backend\app\utils\vectorstore.py =====
from langchain_openai import OpenAIEmbeddings
from langchain_elasticsearch import ElasticsearchStore
from elasticsearch import Elasticsearch
import os

def get_vectorstore(index_name="pdf_chunks"):
    es = Elasticsearch("http://elasticsearch:9200")  # or use ENV
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )
    return ElasticsearchStore(
        es_connection=es,
        index_name=index_name,
        embedding=embeddings,
    )

def get_caption_store():
    return get_vectorstore(index_name="captions")



===== .\backend\app\db.py =====
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from sqlalchemy import Column, String, Integer, Table, TIMESTAMP, ARRAY
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base
import uuid



# DATABASE_URL = "postgresql://myuser:mypassword@postgres:5432/myappdb"
DATABASE_URL = "postgresql://test:test@postgres:5432/testdb"
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)



Base = declarative_base()

class Document(Base):
    __tablename__ = "documents"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    filename = Column(String, nullable=False)
    title = Column(String)
    year = Column(Integer)
    type = Column(String)
    topic = Column(String)
    authors = Column(ARRAY(String))
    isbn = Column(String)
    doi = Column(String)
    publisher = Column(String)
    created_at = Column(TIMESTAMP)

class ImageRecord(Base):
    __tablename__ = "images"

    id = Column(Integer, primary_key=True, index=True)
    book_id = Column(String, index=True)
    source_pdf = Column(String)
    page_number = Column(Integer)
    xref = Column(Integer)
    filename = Column(String, unique=True)
    caption = Column(String)





def get_db():
    db: Session = SessionLocal()
    try:
        yield db
    finally:
        db.close()



===== .\backend\app\main.py =====
from fastapi import FastAPI
from app.routers import health, upload, extract, process, query, query_agent, agent
from app.db import Base, engine  # Ensure engine is correctly configured

# Auto-create tables
Base.metadata.create_all(bind=engine)



app = FastAPI(
    title="My API",
    # docs_url="/backend/docs",
    # redoc_url="/backend/redoc",
    # openapi_url="/backend/openapi.json",
    root_path="/backend"
)

# Optional: If you're mounting routes, align them too
app.include_router(health.router)
app.include_router(upload.router)
app.include_router(extract.router)
app.include_router(process.router)
app.include_router(query.router)
app.include_router(query_agent.router)
app.include_router(agent.router)



===== .\backend\app\schemas.py =====
# app/schemas.py
from pydantic import BaseModel
from typing import Optional, List

class ImageMetadata(BaseModel):
    book_id: str
    source_pdf: str
    page_number: int
    xref: int
    filename: str
    caption: Optional[str] = ""
    embedding: Optional[List[float]] = None  # Optional for later use

    class Config:
        from_attributes = True



===== .\pdf_worker\app\utils\cleaning\clean_text_pipeline.py =====
import fitz
from typing import List
from app.utils.cleaning.header_footer import collect_repeating_lines, remove_repeating_lines
from app.utils.cleaning.page_numbers import detect_page_numbers, remove_page_numbers

def clean_document_text(pdf_path: str) -> List[str]:
    """Returns cleaned text per page (as joined string per page)."""
    doc = fitz.open(pdf_path)
    # pages_text = [page.get_text().splitlines() for page in doc[59:62]]
    pages_text = [page.get_text().splitlines() for page in doc]

    # Step 1: Remove headers & footers
    header_set, footer_set = collect_repeating_lines(pages_text)
    pages_no_headers = remove_repeating_lines(pages_text, header_set, footer_set)

    # Step 2: Remove page numbers
    sequences = detect_page_numbers(pages_text)
    fully_cleaned = remove_page_numbers([page.splitlines() for page in pages_no_headers], sequences)

    return fully_cleaned  # list of strings (one per page)
 



===== .\pdf_worker\app\utils\cleaning\header_footer.py =====
# from collections import defaultdict, Counter
from typing import List #, Dict
# import fitz
from typing import List, Tuple, Set
from rapidfuzz import fuzz



def normalize(line: str) -> str:
    return line.strip().lower()

def detect_repeating_lines_next_pages(
    pages_text: List[List[str]],
    n: int = 5
) -> List[Tuple[List[str], List[str]]]:
    """
    For each page, return (header_lines, footer_lines) if any of them repeat
    exactly in the next 1â€“2 pages.
    """
    results = []

    for i, page in enumerate(pages_text):
        top_lines = [normalize(line) for line in page[:n]]
        bottom_lines = [normalize(line) for line in page[-n:]]

        header_matches = set()
        footer_matches = set()

        for j in [1, 2]:  # lookahead: next page and one after
            if i + j >= len(pages_text):
                continue

            next_page = pages_text[i + j]
            next_top = [normalize(line) for line in next_page[:n]]
            next_bottom = [normalize(line) for line in next_page[-n:]]

            # Exact match for header lines
            for line in top_lines:
                if line and line in next_top:
                    header_matches.add(line)

            # Exact match for footer lines
            for line in bottom_lines:
                if line and line in next_bottom:
                    footer_matches.add(line)

        results.append((
            list(header_matches),
            list(footer_matches)
        ))

    return results



def normalize(line: str) -> str:
    return line.strip().lower()

def collect_repeating_lines(
    pages_text: List[List[str]],
    n: int = 5,
    lookahead: int = 2,
    threshold: int = 100  # exact match = 100, or set to 85 for fuzzy
) -> (Set[str], Set[str]):
    """
    Scans all pages. For each page, checks top/bottom `n` lines against the next `lookahead` pages.
    If any line is repeated, it is added to the global header/footer sets.
    """
    header_candidates = set()
    footer_candidates = set()

    for i, page in enumerate(pages_text):
        top_lines = [normalize(line) for line in page[:n]]
        bottom_lines = [normalize(line) for line in page[-n:]]

        for j in range(1, lookahead + 1):
            if i + j >= len(pages_text):
                break

            next_top = [normalize(line) for line in pages_text[i + j][:n]]
            next_bottom = [normalize(line) for line in pages_text[i + j][-n:]]

            for line in top_lines:
                if line and any(fuzz.ratio(line, other) >= threshold for other in next_top):
                    header_candidates.add(line)

            for line in bottom_lines:
                if line and any(fuzz.ratio(line, other) >= threshold for other in next_bottom):
                    footer_candidates.add(line)

    return header_candidates, footer_candidates




def remove_repeating_lines(
    pages_text: List[List[str]],
    header_set: Set[str],
    footer_set: Set[str],
    n: int = 5
) -> List[str]:
    """
    Removes lines from each page if they match a known header/footer line
    (within top/bottom `n` lines).
    Returns list of cleaned page strings.
    """
    cleaned_pages = []

    for page in pages_text:
        cleaned = []
        for i, line in enumerate(page):
            norm = line.strip().lower()

            is_header_zone = i < n
            is_footer_zone = i >= len(page) - n

            if is_header_zone and norm in header_set:
                continue
            if is_footer_zone and norm in footer_set:
                continue

            cleaned.append(line)

        cleaned_pages.append("\n".join(cleaned))

    return cleaned_pages


# if __name__ == "__main__":
#     # file_path = 'pdfs/black_hole.pdf'
#     # file_path = 'pdfs/de_witte.pdf'
#     file_path = 'pdfs/astronomy.pdf'

#     doc = fitz.open(file_path)
#     pages_text = [page.get_text().splitlines() for page in doc[61:66]]

#     # Step 1: Detect repeated lines
#     header_set, footer_set = collect_repeating_lines(pages_text, n=5)

#     # Step 2: Remove them from the text
#     cleaned_pages = remove_repeating_lines(pages_text, header_set, footer_set, n=5)

#     # Output
#     for i, page in enumerate(cleaned_pages):
#         print(f"\n--- Page {i + 1} ---\n{page}")



===== .\pdf_worker\app\utils\cleaning\page_numbers.py =====
# import fitz  # PyMuPDF
# from difflib import SequenceMatcher
import re
from rapidfuzz import fuzz
import re

def is_arabic_number(s):
    return re.fullmatch(r"\s*\d{1,4}\s*", s) is not None

def is_roman_number(s):
    return re.fullmatch(r"\s*[ivxlcdmIVXLCDM]{1,7}\s*", s) is not None

def roman_to_int(s):
    roman_map = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}
    s = s.upper()
    result = 0
    prev_value = 0
    for char in reversed(s):
        value = roman_map.get(char, 0)
        if value < prev_value:
            result -= value
        else:
            result += value
            prev_value = value
    return result if result > 0 else None

def detect_page_numbers(pages_text, n=3, min_sequence_len=2):
    candidates = []  # list of dicts: {"index": int, "line": str, "pos": "top"/"bottom", "value": int}
    
    for idx, lines in enumerate(pages_text):
        top_lines = lines[:n]
        bottom_lines = lines[-n:] if len(lines) >= n else lines

        for pos, group in [('top', top_lines), ('bottom', bottom_lines)]:
            for line in group:
                stripped = line.strip()
                if is_arabic_number(stripped):
                    value = int(stripped)
                elif is_roman_number(stripped):
                    value = roman_to_int(stripped)
                    if value is None:
                        continue
                else:
                    continue

                candidates.append({
                    "index": idx,
                    "line": line,
                    "pos": pos,
                    "value": value
                })

    # Group by position ("top" or "bottom")
    sequences = []
    for pos in ['top', 'bottom']:
        group = [c for c in candidates if c["pos"] == pos]
        group.sort(key=lambda x: x["index"])

        temp_seq = []
        for i, entry in enumerate(group):
            if not temp_seq:
                temp_seq.append(entry)
            else:
                prev = temp_seq[-1]
                # Allow gaps, but ensure ordering
                if entry["index"] > prev["index"] and entry["value"] > prev["value"]:
                    temp_seq.append(entry)
                elif len(temp_seq) >= min_sequence_len:
                    sequences.append(temp_seq)
                    temp_seq = [entry]
                else:
                    temp_seq = [entry]
        if len(temp_seq) >= min_sequence_len:
            sequences.append(temp_seq)

    return sequences


def remove_page_numbers(pages_text, sequences, n=3):
    cleaned = []
    skip_lines_per_page = {}  # page_index -> list of lines to remove

    for seq in sequences:
        for item in seq:
            page = item["index"]
            line = item["line"].strip()
            skip_lines_per_page.setdefault(page, []).append(line)

    for i, lines in enumerate(pages_text):
        to_remove = skip_lines_per_page.get(i, [])
        cleaned_lines = [line for line in lines if line.strip() not in to_remove]
        cleaned.append("\n".join(cleaned_lines))

    return cleaned



# if __name__ == "__main__":
#     file_path = 'pdfs/black_hole.pdf'
#     # file_path = 'pdfs/de_witte.pdf'

#     doc = fitz.open(file_path)
#     pages_text = [page.get_text().splitlines() for page in doc[61: 66]]
#     # pages_text = [page.get_text().splitlines() for page in doc]

#     # Detect and clean page numbers
#     sequences = detect_page_numbers(pages_text)
#     cleaned_pages = remove_page_numbers(pages_text, sequences)

#     for page in cleaned_pages:
#         print(page)
#         print('---------------------------')



===== .\pdf_worker\app\utils\embedding.py =====
from langchain_openai  import OpenAIEmbeddings  # or `from langchain_openai import OpenAIEmbeddings`
from app.models import TextChunkEmbedding
import os
from dotenv import load_dotenv
import logging
import tiktoken
from typing import List, Callable


MODEL = "text-embedding-3-small"
TOKEN_LIMIT = 300_000
TARGET_BATCH_TOKENS = 250_000  # stay below limit

load_dotenv()

logger = logging.getLogger(__name__)
embedding_model = OpenAIEmbeddings(
    model=MODEL,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# Initialize tokenizer for your embedding model
encoding = tiktoken.encoding_for_model(MODEL)

def estimate_tokens(text: str) -> int:
    return len(encoding.encode(text))

def embed_chunks_streaming(chunks: List[dict], save_fn: Callable[[List[TextChunkEmbedding]], None]):
    logger.info(f"âš¡ Embedding {len(chunks)} chunks in token-capped batches")

    current_batch = []
    current_tokens = 0

    for chunk in chunks:
        chunk_tokens = estimate_tokens(chunk["text"])

        # Start new batch if this chunk would exceed limit
        if current_tokens + chunk_tokens > TARGET_BATCH_TOKENS:
            _process_batch(current_batch, save_fn)
            current_batch = []
            current_tokens = 0

        current_batch.append(chunk)
        current_tokens += chunk_tokens

    # Process any remaining batch
    if current_batch:
        _process_batch(current_batch, save_fn)

    logger.info(f"âœ… All chunks embedded and saved")


def _process_batch(batch: List[dict], save_fn):
    logger.info(f"ðŸ”„ Embedding batch of {len(batch)} chunks")
    texts = [c["text"] for c in batch]

    try:
        vectors = embedding_model.embed_documents(texts)
    except Exception as e:
        logger.error(f"âŒ Failed to embed batch: {e}")
        raise

    results = []
    for chunk, vector in zip(batch, vectors):
        results.append(TextChunkEmbedding(
            chunk_size=chunk["chunk_size"],
            chunk_index=chunk["chunk_index"],
            text=chunk["text"],
            pages=chunk["pages"],
            embedding=vector
        ))

    save_fn(results)
    logger.info(f"ðŸ“¦ Saved {len(results)} embedded chunks")



# load_dotenv()
# embedding_model = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=os.getenv("OPENAI_API_KEY"))

def embed_chunks(chunks: List[dict]) -> List[TextChunkEmbedding]:
    texts = [chunk["text"] for chunk in chunks]
    vectors = embedding_model.embed_documents(texts)

    results = []
    for chunk, vector in zip(chunks, vectors):
        results.append(TextChunkEmbedding(
            chunk_size=chunk["chunk_size"],
            chunk_index=chunk["chunk_index"],
            text=chunk["text"],
            pages=chunk["pages"],
            embedding=vector
        ))
    return results



===== .\pdf_worker\app\utils\embed_captions.py =====
from typing import List
from elasticsearch import Elasticsearch, helpers
from langchain.embeddings import OpenAIEmbeddings
from app.models import ImageMetadata  # Adjust import as needed
import os
from urllib.parse import quote


# Initialize embedding model
embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

# Initialize Elasticsearch client
es = Elasticsearch(hosts=[os.getenv("ELASTICSEARCH_HOST", "http://elasticsearch:9200")])

def embed_and_store_captions(records: List[ImageMetadata], index_name: str = "captions"):
    """
    Embed caption texts from ImageMetadata list and index them in Elasticsearch.
    """
    # Filter records that have a caption
    valid_records = [r for r in records if r.caption and r.caption.strip()]
    if not valid_records:
        print("No valid captions to embed.")
        return

    texts = [r.caption for r in valid_records]
    embeddings = embedding_model.embed_documents(texts)

    payloads = []
    for record, embedding in zip(valid_records, embeddings):
        doc_id = f"{record.book_id}_{record.page_number}_{record.xref}"
        # filename = record.filename

        # minio_path = f"/minio/images/{quote(filename)}"  # or construct full URL if frontend needed

        payloads.append({
            "_index": index_name,
            "_id": doc_id,
            "_source": {
                "book_id": record.book_id,
                "page_number": record.page_number,
                "text": record.caption,
                # "embedding": embedding,
                "vector": embedding,
                "source_pdf": record.source_pdf,
                "xref": record.xref,
                "filename": record.filename,
            }
        })

    helpers.bulk(es, payloads)
    print(f"âœ… Embedded and indexed {len(payloads)} captions into '{index_name}'")



===== .\pdf_worker\app\utils\es.py =====
from elasticsearch import Elasticsearch
import os

es = Elasticsearch("http://elasticsearch:9200")

INDEX_NAME = "pdf_chunks"

def ensure_index():
    if not es.indices.exists(index=INDEX_NAME):
        es.indices.create(index=INDEX_NAME, mappings={
            "properties": {
                "filename": {"type": "keyword"},
                "chunk_size": {"type": "integer"},
                "chunk_index": {"type": "integer"},
                "pages": {"type": "integer"},
                "text": {"type": "text"},
                "vector": {
                    "type": "dense_vector",
                    "dims": 1536,  # adjust to match your embedding model
                    "index": True,
                    "similarity": "cosine"
                }
            }
        })

def save_chunks_to_es(filename: str, chunks: list):
    ensure_index()

    for chunk in chunks:
        doc = {
            "filename": filename,
            "chunk_size": chunk.chunk_size,
            "chunk_index": chunk.chunk_index,
            "pages": chunk.pages,
            "text": chunk.text,
            "vector": chunk.embedding
        }
        doc_id = f"{filename}_{chunk.chunk_size}_{chunk.chunk_index}"
        es.index(index=INDEX_NAME, id=doc_id, document=doc)



===== .\pdf_worker\app\utils\image_extraction.py =====
# import os
import fitz  # PyMuPDF
from PIL import Image
import io
import re
from itertools import chain
# from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict


from app.models import ImageMetadata
from minio import Minio

# --- Setup MinIO ---
minio_client = Minio(
    "minio:9000",
    access_key="minioadmin",
    secret_key="minioadmin123",
    secure=False
)
BUCKET_NAME = "images"


# --- Caption detection regex ---
caption_regex = re.compile(
    r'\b(?:fig(?:s)?\.?|figure(?:s)?|plate(?:s)?|illustration(?:s)?|illus\.?|image(?:s)?|img\.?|'
    r'diagram(?:s)?|diag\.?|chart(?:s)?|graph(?:s)?|photo(?:s)?|phot\.?|table(?:s)?|tab\.?|'
    r'exhibit(?:s)?|ex\.?|panel(?:s)?|graphic(?:s)?|snapshot(?:s)?|rendering(?:s)?|'
    r'infographic(?:s)?|layout(?:s)?)\b',
    re.IGNORECASE
)


def extract_captions_with_bbox(page) -> List[Dict]:
    """Extracts figure captions from the page with bounding boxes."""
    captions = []
    for block in page.get_text("blocks"):
        text = block[4].strip()
        if caption_regex.match(text):
            captions.append({"text": text, "bbox": tuple(block[:4])})
    return captions


def find_closest_caption_to_group(group_bbox: Tuple[float], captions_with_bbox: List[Dict]) -> Dict:
    """Finds the caption closest in vertical position to a group bounding box."""
    group_y_center = (group_bbox[1] + group_bbox[3]) / 2
    return min(captions_with_bbox, key=lambda c: abs((c["bbox"][1] + c["bbox"][3]) / 2 - group_y_center), default=None)


def group_boxes_by_rows(image_boxes: List[Tuple[float]], y_threshold=100) -> List[List[Tuple[float]]]:
    """Groups image boxes by horizontal rows based on vertical proximity."""
    image_boxes.sort(key=lambda b: -b[1])
    groups = [[image_boxes[0]]]
    for box in image_boxes[1:]:
        avg_y_top = sum(b[1] for b in groups[-1]) / len(groups[-1])
        if abs(box[1] - avg_y_top) < y_threshold:
            groups[-1].append(box)
        else:
            groups.append([box])
    return groups


# def save_image(filename: str, image_bytes: bytes, output_dir: str):
#     """Saves an image from bytes to disk."""
#     os.makedirs(output_dir, exist_ok=True)
#     with open(os.path.join(output_dir, filename), "wb") as f:
#         f.write(image_bytes)

def upload_image_to_minio(image_bytes: bytes, filename: str, content_type: str = "image/png"):
    

    stream = io.BytesIO(image_bytes)
    stream.seek(0)  # âœ… Ensure stream is at the beginning

    try:
        minio_client.put_object(
            bucket_name="images",  # make sure this bucket exists
            object_name=filename,
            data=stream,
            length=len(image_bytes),
            content_type=content_type
        )
        print(f"âœ… Uploaded to MinIO: {filename}")
    except Exception as e:
        print(f"âŒ Error uploading {filename} to MinIO: {e}")
        raise


def process_images_and_captions(
    pdf_path: str,
    page_range: List[int],
    book_id: str = "book",
    size_threshold: int = 200 * 200,
    dpi: int = 300,
    padding: int = 20,
    output_dir: str = "output"
) -> List[ImageMetadata]:
    """Processes a range of PDF pages, saving matched images or screenshots and returning metadata."""

    # os.makedirs(output_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    metadata_list = []

    # for page_index in range(len(doc)):
    for page_index in page_range:
        page = doc[page_index]
        layout = page.get_text("dict")
        caption_paragraphs = extract_captions_with_bbox(page)
        caption_count = len(caption_paragraphs)

        image_infos = []
        image_boxes = [
            block["bbox"] for block in layout["blocks"]
            if block.get("type") == 1 and "image" in block
        ]

        for img in page.get_images(full=True):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image = Image.open(io.BytesIO(base_image["image"]))
            area = image.width * image.height

            image_infos.append({
                "bytes": base_image["image"],
                "ext": base_image["ext"],
                "area": area,
                "xref": xref
            })

        image_count = len(image_infos)
        page_label = f"page{page_index + 1}"

        if caption_count == 0:
            print(f"ðŸš« No captions for {page_label}, skipping.")
            continue

        elif image_count == caption_count:
            print(f"ðŸ–¼ï¸ Matched images and captions on {page_label}, saving large images.")
            for i, info in enumerate(image_infos):
                if info["area"] >= size_threshold:
                    filename = f"{book_id}_page{page_index+1:03}_xref{info['xref']}.{info['ext']}"
                    # save_image(filename, info["bytes"], output_dir)
                    # upload_image_to_minio(info["bytes"], filename)
                    try:
                        upload_image_to_minio(
                                image_bytes=info["bytes"],
                                filename=filename,
                                content_type=f"image/{info['ext']}"
                            )
                    except Exception as e:
                        print(f"âŒ Skipping image due to error: {e}")
                        continue

                    caption_text = caption_paragraphs[i]["text"] if i < caption_count else ""
                    metadata_list.append(ImageMetadata(
                        book_id=book_id, source_pdf=pdf_path, page_number=page_index + 1, xref=info["xref"], filename=filename, caption=caption_text
                    ))
                    print(f"âœ… Saved: {filename}")

        elif image_count > caption_count and image_boxes:
            print(f"ðŸ“¸ More images than captions on {page_label}, taking grouped screenshots.")
            groups = group_boxes_by_rows(image_boxes)
            all_boxes = list(chain.from_iterable(groups))

            if len(groups) > caption_count:
                new_groups = groups[:caption_count - 1]
                new_groups.append(list(chain.from_iterable(groups[caption_count - 1:])))
                groups = new_groups

            assert sorted(map(tuple, chain.from_iterable(groups))) == sorted(map(tuple, all_boxes)), f"âŒ Box mismatch on {page_label}"

            zoom = dpi / 72
            mat = fitz.Matrix(zoom, zoom)

            for i, group in enumerate(groups):
                x0 = min(b[0] for b in group) - padding
                y0 = min(b[1] for b in group) - padding
                x1 = max(b[2] for b in group) + padding
                y1 = max(b[3] for b in group) + padding

                rect = fitz.Rect(
                    max(x0, page.rect.x0),
                    max(y0, page.rect.y0),
                    min(x1, page.rect.x1),
                    min(y1, page.rect.y1)
                )

                pix = page.get_pixmap(matrix=mat, clip=rect)
                # filename = f"{book_id}_page{page_index+1:03}_group{i+1}.png"
                # pix.save(os.path.join(output_dir, filename))
                img_bytes = pix.tobytes("png")
                filename = f"{book_id}_page{page_index+1:03}_group{i+1}.png"
                upload_image_to_minio(img_bytes, filename)

                closest_caption = find_closest_caption_to_group((x0, y0, x1, y1), caption_paragraphs)
                caption_text = closest_caption["text"] if closest_caption else ""
                metadata_list.append(ImageMetadata(
                        book_id=book_id, source_pdf=pdf_path, page_number=page_index + 1, xref=-1, filename=filename, caption=caption_text
                    ))
                
                print(f"ðŸ“· Saved screenshot: {filename}")

        else:
            print(f"âš ï¸ Unexpected case for {page_label}, skipping.")

    print("âœ… All pages processed.")
    return metadata_list




# --- Example run ---
# if __name__ == "__main__":
#     file_path = 'pdfs/black_hole.pdf'
#     # file_path = 'pdfs/astronomy.pdf'
#     # file_path = 'pdfs/de_witte.pdf'
#     # result = process_page_smart(pdf_path=file_path, page_range=(38, 55, 56, 57, 58, 59, 48), book_id="astronomy")
#     # result = process_page_smart(pdf_path=file_path, page_range=(56,), book_id="astronomy")
#     result = process_page_smart(pdf_path=file_path, page_range=(38, 55, 56, 57), book_id="astronomy")
#     # result = process_page_smart(pdf_path=file_path, book_id="astronomy")
    
#     for entry in result:
#         print(asdict(entry))  # This shows the metadata per image/screenshot



===== .\pdf_worker\app\utils\metadata.py =====
import fitz  # PyMuPDF
from langchain_openai import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
import os
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from app.models import DocumentMetadata




# --- Metadata Extraction ---
def get_doc_info(file_path):
    doc = fitz.open(file_path)
    num_pages = len(doc)

    N = 10
    page_indices = list(range(min(N, num_pages))) + list(range(max(num_pages - N, 0), num_pages))

    year_pattern = r"(?:19|20)\d{2}"
    candidate_pages = []

    for i in page_indices:
        text = doc[i].get_text()
        candidate_pages.append(text)
    combined_text = "\n---\n".join(candidate_pages)



    # Load API key
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=openai_api_key)

    parser = PydanticOutputParser(pydantic_object=DocumentMetadata)
    prompt = PromptTemplate(
        template="Extract the metadata from this text:\n\n{text}\n\n{format_instructions}",
        input_variables=["text"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    chain = prompt | llm | parser

    try:
        result = chain.invoke({"text": combined_text})
    except Exception as e:
        print("âŒ Parsing failed:", e)
        return None


    return result







===== .\pdf_worker\app\utils\pdf_pipeline.py =====
import fitz  # PyMuPDF
from typing import List
from app.utils.image_extraction import process_images_and_captions
from app.utils.cleaning.header_footer import collect_repeating_lines, remove_repeating_lines
from app.utils.cleaning.page_numbers import detect_page_numbers, remove_page_numbers
from app.utils.text_chunker import chunk_text
from app.utils.embedding import embed_chunks_streaming
from app.utils.embed_captions import embed_and_store_captions
from app.utils.es import save_chunks_to_es
from app.models import ImageMetadata
from app.utils.cleaning.clean_text_pipeline import clean_document_text



import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_pdf(file_path: str, book_id: str, source_pdf: str):
    print(f"ðŸ“˜ Starting full processing for: {source_pdf}")

    # Step 1: Cleaned full-page text using centralized logic
    cleaned_pages = clean_document_text(file_path)

    # Step 2: Extract and save images + captions
    doc = fitz.open(file_path)
    page_range = list(range(len(doc)))  # full document
    image_records: List[ImageMetadata] = process_images_and_captions(
        pdf_path=file_path,
        page_range=page_range,
        book_id=book_id
    )

    # Step 3: Remove caption lines from cleaned text
    for img in image_records:
        if img.caption.strip():
            page_idx = img.page_number - 1
            if 0 <= page_idx < len(cleaned_pages):
                cleaned_pages[page_idx] = "\n".join([
                    line for line in cleaned_pages[page_idx].splitlines()
                    if img.caption.strip() not in line.strip()
                ])

    # Step 4: Chunk the cleaned text
    # chunks = chunk_text(cleaned_pages, chunk_sizes=[200, 400, 800, 1600])
    chunks = chunk_text(cleaned_pages, chunk_sizes=[400, 1600])
    logger.info(f"ðŸ”– Total chunks created: {len(chunks)}")

    # Step 5: Embed and save chunks
    embed_chunks_streaming(
        chunks,
        save_fn=lambda batch: save_chunks_to_es(source_pdf, batch)
    )

    # Step 6: Embed and store captions
    embed_and_store_captions(image_records)

    print(f"âœ… Finished processing: {source_pdf}")

# def process_pdf(file_path: str, book_id: str, source_pdf: str):
#     print(f"ðŸ“˜ Starting full processing for: {source_pdf}")

#     # Step 1: Load PDF
#     doc = fitz.open(file_path)
#     # pages_text = [page.get_text().splitlines() for page in doc]
#     # pages_text = [page.get_text().splitlines() for page in doc[59:62]]
#     cleaned_pages = clean_document_text(file_path)




#     # Step 2: Extract and save images + captions
#     page_range = list(range(len(doc)))  # You may restrict this if needed
#     image_records: List[ImageMetadata] = process_images_and_captions(
#         pdf_path=file_path,
#         page_range=page_range,
#         book_id=book_id
#     )

#     # Step 3: Remove caption lines from the text (by exact match per page)
#     for img in image_records:
#         if img.caption.strip():
#             page_idx = img.page_number - 1
#             if 0 <= page_idx < len(pages_text):
#                 pages_text[page_idx] = [
#                     line for line in pages_text[page_idx]
#                     if img.caption.strip() not in line.strip()
#                 ]

#     # Step 4: Remove headers and footers
#     logger.info(f"--- 1 ---")
#     header_set, footer_set = collect_repeating_lines(pages_text, n=5, lookahead=2, threshold=95)
#     pages_text = remove_repeating_lines(pages_text, header_set, footer_set, n=5)
#     logger.info(f"--- 2 ---")
#     # Step 5: Remove page numbers
#     page_number_sequences = detect_page_numbers(pages_text)
#     pages_text = remove_page_numbers(pages_text, page_number_sequences)
#     logger.info(f"--- 3 ---")
#     # Step 6: Normalize and chunk text
#     cleaned_pages = ["\n".join(lines) for lines in pages_text]
#     chunks = chunk_text(cleaned_pages, chunk_sizes=[200, 400, 800, 1600])
#     # chunks = chunk_text(cleaned_pages, chunk_sizes=[400, 800])
#     logger.info(f"--- 4 ---")
#     # Step 7: Embed and save text chunks
#     # embedded_chunks = embed_chunks(chunks)
#     embed_chunks_streaming(
#         chunks,
#         save_fn=lambda batch: save_chunks_to_es(source_pdf, batch),
#         # batch_size=1  # or 10 for better throughput
#     )

#     logger.info(f"--- 5 ---")
#     # save_chunks_to_es(source_pdf, embedded_chunks)
#     # logger.info(f"--- 6 ---")
#     # Step 8: Embed and save captions
#     embed_and_store_captions(image_records)

#     print(f"âœ… Finished processing: {source_pdf}")



===== .\pdf_worker\app\utils\pdf_reader.py =====
from langchain_community.document_loaders import PyMuPDFLoader

from minio import Minio
import os


def read_pdf_from_minio(filename: str, bucket: str = "uploads") -> list:
    
    minio_client = Minio(
        "minio:9000",
        access_key="minioadmin",
        secret_key="minioadmin123",
        secure=False
    )

    # Download file from MinIO
    local_path = f"/tmp/{filename}"
    minio_client.fget_object(bucket, filename, local_path)

    # Extract text using LangChain loader
    loader = PyMuPDFLoader(local_path)
    documents = loader.load()
    return documents 


def download_from_minio(filename: str, bucket: str = "uploads") -> list:
    
    minio_client = Minio(
        "minio:9000",
        access_key="minioadmin",
        secret_key="minioadmin123",
        secure=False
    )

    # Download file from MinIO
    local_path = f"/tmp/{filename}"
    minio_client.fget_object(bucket, filename, local_path)

    return local_path







===== .\pdf_worker\app\utils\text_chunker.py =====
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict


def normalize_page_text(page: str) -> str:
    """
    Converts line-based page text to normalized paragraph text.
    """
    return ' '.join(line.strip() for line in page.splitlines() if line.strip())


def get_page_offsets(pages: List[str]) -> List[Dict]:
    """
    Returns a list of page start/end offsets for mapping chunks to pages.
    """
    page_offsets = []
    offset = 0
    for i, page in enumerate(pages, start=1):
        length = len(page)
        page_offsets.append({
            "page": i,
            "start": offset,
            "end": offset + length
        })
        offset += length + 2  # Account for \n\n joining
    return page_offsets


def map_chunk_to_pages(start: int, end: int, page_offsets: List[Dict]) -> List[int]:
    """
    Returns a list of pages overlapped by the chunk.
    """
    return [p["page"] for p in page_offsets if p["end"] >= start and p["start"] <= end]


def chunk_text(cleaned_pages: List[str], chunk_sizes: List[int]) -> List[Dict]:
    """
    Splits cleaned PDF text into multi-size overlapping chunks with page tracking.
    """
    # Step 1: Normalize
    normalized_pages = [normalize_page_text(page) for page in cleaned_pages]
    full_text = "\n\n".join(normalized_pages)
    page_offsets = get_page_offsets(normalized_pages)

    all_chunks = []

    for size in chunk_sizes:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=size,
            chunk_overlap=int(size * 0.2),
            separators=["\n\n", ".", "!", "?", "\n", " ", ""]
        )
        docs = splitter.create_documents([full_text])

        cursor = 0  # tracks last match
        for i, doc in enumerate(docs):
            chunk_text = doc.page_content
            start = full_text.find(chunk_text, cursor)
            if start == -1:
                # fallback to brute match
                start = full_text.index(chunk_text)
            end = start + len(chunk_text)
            cursor = end

            pages = map_chunk_to_pages(start, end, page_offsets)

            all_chunks.append({
                "chunk_size": size,
                "chunk_index": i,
                "text": chunk_text,
                "pages": pages
            })

    return all_chunks



===== .\pdf_worker\app\main.py =====
from fastapi import FastAPI, HTTPException
from app.utils.pdf_reader import read_pdf_from_minio, download_from_minio

from app.utils.metadata import get_doc_info
# from app.utils.image_extraction import extract_images_and_captions  # to implement
from app.models import DocumentMetadata, ImageMetadata

from typing import Optional, List
from dataclasses import dataclass, asdict
from app.utils.image_extraction import process_images_and_captions
# import fitz  # PyMuPDF
from app.utils.cleaning.clean_text_pipeline import clean_document_text
# from app.utils.text_chunker import chunk_text


# from app.utils.cleaning.clean_text_pipeline import clean_document_text
from app.utils.text_chunker import chunk_text
from app.utils.embedding import embed_chunks
from app.models import TextChunkEmbedding
from app.utils.es import save_chunks_to_es



app = FastAPI(root_path="/pdfworker")

@app.post("/extract/{filename}")
def extract_pdf(filename: str):
    try:
        docs = read_pdf_from_minio(filename)
        return {
            "filename": filename,
            "chunks": len(docs),
            "pages": [doc.page_content for doc in docs],
            "preview": docs[0].page_content[:200] if docs else "No content"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/metadata/{filename}", response_model=DocumentMetadata)
def extract_metadata(filename: str):
    try:
        local_path = download_from_minio(filename)
        return get_doc_info(local_path)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# @app.post("/images/{filename}", response_model=List[ImageMetadata])
# def extract_images(filename: str):
#     try:
#         return extract_images_and_captions(filename)
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

@app.post("/images/{filename}", response_model=List[ImageMetadata])
def extract_images(filename: str):
    try:
        local_path = download_from_minio(filename)
        # doc = fitz.open(local_path)
        # page_range = list(range(len(doc)))  # Process all pages; change if needed
        page_range=(38, 55, 56, 57)
        return process_images_and_captions(local_path, page_range, book_id=filename.split("_")[0])
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))




@app.post("/clean_chunks/{filename}")
def process_and_chunk_pdf(filename: str):
    try:
        local_path = download_from_minio(filename)

        cleaned_pages = clean_document_text(local_path)

        chunks = chunk_text(cleaned_pages, chunk_sizes=[200, 400, 800, 1600])
        

        return {
            "status": "success",
            "chunk_sizes": list(set(c["chunk_size"] for c in chunks)),
            "chunks": chunks,
            "page_count": len(set(p for chunk in chunks for p in chunk["pages"]))
        }


    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



@app.post("/embed_chunks/{filename}", response_model=List[TextChunkEmbedding])
def process_clean_embed_chunks(filename: str):
    try:
        local_path = download_from_minio(filename)

        # Step 1: Clean
        cleaned_pages = clean_document_text(local_path)

        # Step 2: Chunk
        # chunks = chunk_text(cleaned_pages, chunk_sizes=[200, 400, 800, 1600])
        chunks = chunk_text(cleaned_pages, chunk_sizes=[800, 1600])

        # Step 3: Embed
        embedded = embed_chunks(chunks)

        # âœ… Step 4: Save to ES
        save_chunks_to_es(filename, embedded)

        return embedded

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


from app.utils.pdf_pipeline import process_pdf

@app.post("/process/full/{filename}")
def full_pdf_pipeline(filename: str):
    try:
        local_path = download_from_minio(filename)
        book_id = filename.split("_")[0]
        process_pdf(local_path, book_id, filename)
        return {
            "status": "âœ… done",
            "filename": filename
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



===== .\pdf_worker\app\models.py =====
from pydantic import BaseModel, Field
from enum import Enum
from typing import Optional, List
from dataclasses import dataclass


# --- Enum Definitions ---
class TypeCategory(str, Enum):
    book = "book"
    article = "article"
    thesis = "thesis"
    report = "report"
    unknown = "unknown"


class TopicCategory(str, Enum):
    fiction = "fiction"
    proza = "proza"
    poetry = "poetry"
    history = "history"
    physics = "physics"
    mathematics = "mathematics"
    computer_science = "computer science"
    politics = "politics"
    chemistry = "chemistry"
    biology = "biology"
    biography = "biography"
    law = "law"
    philosophy = "philosophy"
    religion = "religion"
    journalism = "journalism"
    medical = "medical"
    economy = "economy"
    finance = "finance"
    unknown = "unknown"


# --- Pydantic Schema ---
class DocumentMetadata(BaseModel):
    title: str = Field(..., description="the title of the book, might be black hole ")
    year: int = Field(..., description="The publication year (e.g. 2020)")
    type: TypeCategory = Field(..., description="The type of document")
    topic: TopicCategory = Field(..., description="The topic of document")
    authors: Optional[List[str]] = Field(None, description="All the authors")
    isbn: Optional[str] = Field(None, description="The ISBN number, if available")
    doi: Optional[str] = Field(None, description="The DOI, if available")
    publisher: Optional[str] = Field(None, description="The publisher of the book, if available")




class ImageMetadata(BaseModel):
    book_id: str
    source_pdf: str
    page_number: int
    xref: int
    filename: str
    caption: str = ""
    embedding: List = None



class TextChunkEmbedding(BaseModel):
    chunk_size: int
    chunk_index: int
    text: str
    pages: List[int]
    embedding: List[float]



